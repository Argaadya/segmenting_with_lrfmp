---
title: "Segmenting Customer with LRFMP Model"
author: "Arga Adyatama"
date: 2023-09-30
format: 
  html:
    toc: true
    toc-depth: 3
    toc-location: left
    code-fold: show
    code-line-numbers: true
    theme: journal
editor: visual
---

# Introduction

The main purpose of this article is to learn how to identify different customer segments in retail industry using **K-Means** clustering method. Customer segmentation enables companies to divide customers into distinct and internally homogeneous groups and interact with each customer segment separately. Moreover, customer segmentation is a critical success factor for understanding behavior of different groups of customers and evaluating their value.

This article is inspired by a publication by [Peker et al.](https://www.emerald.com/insight/content/doi/10.1108/MIP-11-2016-0210/full/html) who proposed an LRFMP (Length, Recency, Frequency, Periodicity) model for classifying customers in the grocery retail industry. All source code and dataset for this article are provided on [my github repo](https://github.com/Argaadya/segmenting_with_lrfmp).

# Library

The following are the required packages that will be used throughout the article. It consists of packages for data wrangling, clustering, and data visualization.

```{python}
# Data Wrangling
import pandas as pd
import numpy as np

# Visualization
import matplotlib.pyplot as plt

# Clustering
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score

# PCA for visualizing cluster
from sklearn.decomposition import PCA

pd.set_option('display.max_columns', None)

```

# Data Understanding

The dataset is acquired from [Kaggle](https://www.kaggle.com/datasets/atharvaarya25/kpmg-dummy-data). It consists of transaction records and customer data from Sprocket Central Pty Ltd, a medium size bikes & cycling accessories organisation. For the sake of simplicity, we will only segment customers based on their transaction record and ignore the customer demographics.

## Read Data

First we will read and inspect the transaction data.

```{python}
df_order = pd.read_csv("data/transaction.csv")

df_order.info()
```

Data description:

-   `transation_id`: unique identifier for each transaction
-   `product_id`: unique identifier for each sold product
-   `customer_id`: unique identifier for each customer
-   `transaction_date`: date of transaction
-   `online_order`: whether the transaction is done online or offline
-   `order_status`: the status of the order (approved or canceled)
-   `brand`: the brand of the product
-   `product_line`: the product line category of the product
-   `product_class`: the product class of the product
-   `product_size`: the size of the product
-   `list_price`: the listed price during transaction
-   `standard_cost`: the standard cost of the product
-   `product_first_sold_date`: the first time product is sold

## Inspect Data

Let's check the summary of each column from the data to see if there is anything unusual.

```{python}
df_order.info()
```

There are around 300 transaction with missing order status (`NA`). Next, we will check the order status of each transacton and see how many order are approved, assuming that an approved order is a completed transaction and the company gain revenue from this transaction.

```{python}
df_order.groupby('order_status').agg({'order_status' : 'count'})
```

Most of the transaction is approved with small number of cancelled transaction.

# Data Preparation

## Data Cleansing

We will exclude the cancelled transaction from the data. We will also transform the transaction date into a proper date data type.

```{python}
df_clean = df_order[ df_order.order_status == "Approved"].copy()
df_clean['transaction_date'] = pd.to_datetime(df_clean['transaction_date'], format = "%d/%m/%Y")

df_clean.info()
```

## LRFMP Model

We will prepare the data for creating the **LRFMP Model**. The model is a development from the traditional RFM model with the addition of 2 new variables: `Length` and `Periodicity`. Below is the definition of each variable from Peker et al.:

-   `Length`: This feature is the time interval, in days, between the customer's first and last visits. It shows the customer loyalty, and the higher the length is, the more loyal a customer is.
-   `Recency`: The average of number of days between the dates of the customer's N recent visits and the last date of the observation period.
-   `Length`: Frequency refers to the customer's total number of visits during the observation period. The higher the frequency is, the higher the customer loyalty becomes.
-   `Monetary`: Monetary refers to the average amount of money spent per visit by the customer during the observation period and reflects the contribution of the customer to the revenue of a company.
-   `Periodicity`: The standard deviation of the customer's inter-visit times. Periodicity indicates the tendency of a customer's visits to occur at regular intervals.

You may create the LRFMP model based on the above definition. However, for easier interpretation I will change the definition for the following metrics:

-   `Recency`: The number of days between customer's last visit (N = 1) and the last date of observation period
-   `Periodicity`: The median number of days of customer's inter-visit times. This will change the perspective that instead of looking at the regularity of customer visit time, we will look at the average inter-visit time between customer visits.

First let's calculate the `Length`, `Recency`, `Frequency`, and `Monetary` metrics.

```{python}
df_agg = df_clean.groupby('customer_id').agg({'transaction_date' : ['nunique', 'min', 'max'], 
                                              'list_price' : 'sum'
                                              }).reset_index()

df_agg.columns = df_agg.columns.map('_'.join)
df_agg.rename({'transaction_date_nunique' : 'count_visit',
               'transaction_date_min' : 'first_visit',
               'transaction_date_max' : 'last_visit',
               'list_price_sum' : 'sales',
               'customer_id_' : 'customer_id'
              }, 
                inplace= True, axis = 1
                )

df_agg.head()
```


```{python}
df_agg['frequency'] = df_agg['count_visit']
df_agg['monetary'] = df_agg['sales']/df_agg['frequency']
df_agg['recency'] = (df_clean['transaction_date'].max() - df_agg['last_visit']).dt.days
df_agg['length'] = (df_agg['last_visit'] - df_agg['first_visit']).dt.days

df_agg[['customer_id', 'length', 'recency', 'frequency', 'monetary']].head()
```

Next calculate the `Periodicity` metric.

```{python}
df_agg_2 = df_clean[['customer_id', 'transaction_date']].drop_duplicates().sort_values(['customer_id', 'transaction_date'])

df_agg_2['lag_date'] = df_agg_2.groupby('customer_id')['transaction_date'].shift(1)

# Drop NA
df_agg_2.dropna(subset = ['lag_date'], inplace = True)

df_agg_2['interval_day'] = (df_agg_2['transaction_date'] - df_agg_2['lag_date']).dt.days

df_agg_2 = df_agg_2.groupby('customer_id').agg({'interval_day' : 'median'}).reset_index()

df_agg_2.rename({'interval_day' : 'periodicity'}, axis = 1, inplace = True)

df_agg_2.head()
```

Finally, we will combine both dataframe to gain the complete LRFM value for each customer.

```{python}
df_final = df_agg.merge(df_agg_2, how = 'left', on = 'customer_id')

df_final = df_final[['customer_id', 'length', 'recency', 'frequency', 'monetary', 'periodicity']]

df_final.set_index('customer_id', inplace = True)

df_final.head()
```

## Single Purchase Customer

Let's check the summary of the data.

```{python}
df_final.info()
```

As we can see, there are 50 customers with missing `Periodicity` values. This indicate that the customer only visit the store once (`Frequency` = 1) and not yet return for the second purchase.


```{python}
df_final[ df_final.periodicity.isnull() ].head()
```

We can exclude this customers from further analysis since we don't have enough information for their transaction histories. From a marketing perspective, we can do a separate campaign for this customer to do their second purchase.

```{python}
df_final = df_final.dropna()

df_final.info()
```

## Scaling Variables

Clustering algorithm will calculate the distance between data point, commonly using the euclidean distance:

$$
distance(a, b) = \sqrt {\Sigma_{i=1}^n (a_i - b_i)^2}
$$

If we feed the data directly into the clustering algorithm, the distance from the `Monetary` variable will have significant influence compared to the `Frequency` variable since the `Monetary` variable has wider range of value. Therefore, the algorithm will give better result if all variables has the same scale. To address this problem, we will scale the data using the standardize normal distribution.

$$
Z = \frac{x - \mu}{\sigma}
$$

```{python}
scaler = StandardScaler()

df_scaled_clean = scaler.fit_transform(df_final)

df_scaled_clean[0:5]
```

# Segmenting Customers

## Clustering with LRFMP

### Determine Number of Clusters

We will start segmenting customer using the **K-Means** algorithm. If you are not familiar with the algorithm, you may check the [video from StatQuest](https://www.youtube.com/watch?v=4b5d3muPQmA&ab_channel=StatQuestwithJoshStarmer) for a complete step-by-step process of determining the cluster of each data point.

The first thing we do is to determine the optimal number of cluster, which can be evaluated using different metrics. The most common metrics is using the *within sum of square (wss)* which represent the distance between each data point to their respective cluster centroid. The other metric is using **silhouette score**.

> Silhouette Score is a metric to evaluate the performance of clustering algorithm. It uses compactness of individual clusters(intra cluster distance) and separation amongst clusters (inter cluster distance) to measure an overall representative score of how well our clustering algorithm has performed.

The value of the Silhouette score ranges from -1 to 1 with the following interpretation:

-   silhouette score = 1: Points are perfectly assigned in a cluster and clusters are easily distinguishable.
-   silhouette score = 0: Clusters are overlapping.
-   silhouette score = -1: Points are wrongly assigned in a cluster.

Here we will show both the WSS and the silhouette score for each number of clusters.

```{python}
sil_score = np.zeros(20)
wss_score = np.zeros(20)

for i in range(1, 20):
  
  x = i + 1
  
  # Cluster the data
  clust_temp = KMeans(n_clusters=x, random_state=123, n_init="auto").fit(df_scaled_clean)
  labels = clust_temp.labels_
  
  # Calculate Silhouette Score
  sil_score[i] = silhouette_score(df_scaled_clean, labels, metric = 'euclidean')
  
  # Calculate Within Sum of Square
  wss_score[i] = clust_temp.inertia_
  
trial_clust = pd.DataFrame({'n_cluster' : pd.to_numeric(list(range(1, 20))) + 1,
                            'silhouette_score' : sil_score[1:],
                            'wss_score' : wss_score[1:]
                            })

# Highlight Optimal Number of Cluster
p_1 = trial_clust[ trial_clust.silhouette_score == trial_clust.silhouette_score.max()]
```

First let's visualize the result of silhouette score. The x-axis shows the number of cluster while the y-axis shows the respective silhouette score. Based on the result, if we want to determine the number of cluster using the silhouette score, we will choose the number of cluster = 3 since it has the highest silhouette score.

```{python}
plt.plot(trial_clust.n_cluster.astype("str"), trial_clust.silhouette_score)
plt.scatter(p_1.n_cluster.astype("str"), p_1.silhouette_score, s = 100)
plt.xlabel("Number of Cluster") 
plt.ylabel("Silhouette Score")

plt.show()
plt.close()
```

The optimal number of cluster using WSS is a bit more tricky since there is no exact value to be chosen. Instead we will have to look for the *elbow* or the point where the decrease in WSS is considered to be no longer significant. The following figure is an example of determining the number of clustering using elbow method.

![](asset/The-elbow-method-of-k-means.png)

Below are the result of our clustering. There is no clear elbow since the curve is not so steep compared to the previous figure. You may choose the number of cluster between 4 to 6 as the optimal number of cluster since after number of cluster = 6 the decrease in WSS is starting to get very small compared to the initial decrease from number of cluster = 2 to 3.

```{python}
plt.plot(trial_clust.n_cluster.astype("str"), trial_clust.wss_score)
plt.scatter(trial_clust.n_cluster.astype("str"), trial_clust.wss_score, s = 10)
plt.xlabel("Number of Cluster") 
plt.ylabel("Within Sum of Square")

plt.show()
plt.close()
```



### Clustering

Determining the number of optimal clusters is always hard, especially for higher dimension dataset with many variables. It also influenced by our domain knowledge regarding the business implication of the result. Thus, one should not have to stick to a single number but rather focus more on the result of the cluster. For this use case, we will use **number of cluster = 4** since it will give more simple result and easier interpretation since we don't have to explain a large number of cluster. It will also give us more room for grouping the outlier or high/low performing customers together.

```{python}
k_clust = KMeans(n_clusters=4, random_state=123, n_init="auto").fit(df_scaled_clean)


list_cluster = pd.DataFrame({'customer_id' : list(df_final.index), 'cluster' : k_clust.labels_})

df_out = df_final.reset_index().merge(list_cluster, how = 'left', on = 'customer_id')

df_out.head()
```


### Profiling Customers

The next step after we assign cluster segment for each customer is profiling the different segments and identify the difference between them. We will get the centroid of the mean of each variables from each cluster to profile the member of each cluster.

```{python}
summary_1 = df_out.groupby('cluster').agg({'customer_id' : 'nunique',
                                           'length' : 'mean',
                                           'recency' : 'mean',
                                           'frequency' : 'mean',
                                           'monetary' : 'mean',
                                           'periodicity' : 'mean'
                                           })
                                           
summary_1['percent_member'] = summary_1['customer_id']/summary_1['customer_id'].sum()

summary_1.sort_values('customer_id', ascending = False)
```


Let's try to characterize each cluster based on their LRFMP values. You can assign different cluster name based on your own interpretation.

**Cluster 3: Most Loyal**

Cluster 3 has high number of member, with 34% of customers belong to this cluster. They are identified by the highest `Frequency` of visit and also the most `Recent` member to visit the store. They are also the most loyal indicated by the `Length` variable. They will on average visit our store once a month based on the `Periodicity` value. We must keep this segment of customers since they are the most valuable.

**Cluster 1: Regular**

Cluster 1 is indicated by their high `Frequency` of visits and quite loyal although has lower `Length` compared to the cluster 3. They wil visit our store every 2 months on average based on the `Periodicity`value. They also has the lowest `Monetary` values compared to other clusters.

**Cluster 0: Hibernating**

Cluster 0 is indicated by their `Recency` which shows that their last visit to our store was around 4 months ago. This cluster may require special treatment to prevent them from churning to other competitor. Although they have visited us around 4 times based on their `Frequency`, they are also the least loyal as shown by the value of `Length`.

**Cluster 2: Seasonal**

Cluster 2 has the lowest number of member, with only 10% of customers belong to this cluster. They are identified by the lowest `Frequency` of visit (only 3 times) and on average only visit our store around every 4-5 months based on the `Periodicity` value.


## Clustering with LFRMP and Product Preferences

### Product Preferences

We have done segmenting customers using the LRFMP model on the previous section. Based on the result, we can also identify that there are several segments of customer. On this section, we will try to add more variables to consider, namely the product preference. By knowing what is the preferred product for each segment, we can provide better and personalized promotion and campaign to approach the customer.

Let's once again look at the initial transaction data.

```{python}
df_clean.head()
```

There are several information about the product purchased: `brand`, `product_line`, `product_classs`, and `product_size`.

Let's try to check the product preference based on the product line. We will look at the unique value of the product line.

```{python}
df_clean['product_line'].unique()
```

There is a product line that has nan value. We will transform this into *Other*.

```{python}
df_clean['product_line'] = df_clean['product_line'].fillna(value = 'other')

df_clean['product_line'].unique()
```

To represent the product preference of each customer, we will get the `Monetary` spending for each product line. Knowing this information will help us understand what line of product each segment prefer to buy at what price range.

```{python}
df_agg_3 = df_clean.groupby(['customer_id', 'product_line']).agg({'transaction_date' : 'nunique', 'list_price' : 'sum'})

df_agg_3['monetary'] = df_agg_3['list_price']/df_agg_3['transaction_date']

df_agg_3.drop(['transaction_date', 'list_price'], axis = 1, inplace = True)
df_agg_3.reset_index(inplace = True)

df_agg_3 = df_agg_3.pivot(columns = 'product_line', values = 'monetary', index = 'customer_id' )
df_agg_3.reset_index(inplace = True)

df_agg_3
```

Since most customers will not buy all product line, we will replace the missing value with 0.

```{python}
num_column = df_agg_3.select_dtypes(include = np.number).columns

df_agg_3[num_column] = df_agg_3[num_column].fillna(0)

df_agg_3
```

Let's combine this information to our previous LRFMP model.

```{python}
df_final_2 = df_final.merge(df_agg_3, how = 'left', on = 'customer_id')
df_final_2.set_index('customer_id', inplace = True)

df_final_2.head()
```

### Scaling Variables


```{python}
scaler = StandardScaler()

df_scaled_clean_2 = scaler.fit_transform(df_final_2)

df_scaled_clean_2[0:5]
```

### Determine Number of Clusters

Let's try to find the optimal number of clusters for the new dataset.

```{python}
sil_score = np.zeros(20)
wss_score = np.zeros(20)

for i in range(1, 20):
  
  x = i + 1
  
  # Cluster the data
  clust_temp = KMeans(n_clusters=x, random_state=123, n_init="auto").fit(df_scaled_clean_2)
  labels = clust_temp.labels_
  
  # Calculate Silhouette Score
  sil_score[i] = silhouette_score(df_scaled_clean_2, labels, metric = 'euclidean')
  
  # Calculate Within Sum of Square
  wss_score[i] = clust_temp.inertia_
  
trial_clust = pd.DataFrame({'n_cluster' : pd.to_numeric(list(range(1, 20))) + 1,
                            'silhouette_score' : sil_score[1:],
                            'wss_score' : wss_score[1:]
                            })

# Highlight Optimal Number of Cluster
p_1 = trial_clust[ trial_clust.silhouette_score == trial_clust.silhouette_score.max()]
```

Let's visualize the result of silhouette score. The x-axis shows the number of cluster while the y-axis shows the respective silhouette score.

```{python}
plt.plot(trial_clust.n_cluster.astype("str"), trial_clust.silhouette_score)
plt.scatter(p_1.n_cluster.astype("str"), p_1.silhouette_score, s = 100)
plt.xlabel("Number of Cluster") 
plt.ylabel("Silhouette Score")

plt.show()
plt.close()
```


Let's visualize the WSS value of each number of cluster.


```{python}
plt.plot(trial_clust.n_cluster.astype("str"), trial_clust.wss_score)
plt.scatter(trial_clust.n_cluster.astype("str"), trial_clust.wss_score, s = 10)
plt.xlabel("Number of Cluster") 
plt.ylabel("Within Sum of Square")

plt.show()
plt.close()
```


### Clustering

Based on the silhouette score, the number of optimal cluster = 3 while from WSS we may choose cluster = 9 since there is a big decrease from 8 to 9. We wil use the number of cluster = 8.


```{python}
k_clust = KMeans(n_clusters=9, random_state=123, n_init="auto").fit(df_scaled_clean_2)

list_cluster = pd.DataFrame({'customer_id' : list(df_final_2.index), 'cluster' : k_clust.labels_})

df_out = df_final_2.reset_index().merge(list_cluster, how = 'left', on = 'customer_id')

df_out.head()
```


### Profiling Customers

The next step after we assign cluster segment for each customer is profiling the different segments and identify the difference between them. For the LRFMP metrics we will calculate the mean while for the monetary value for each product line we will use median. The reason for using median is that since median reflect the middle value of the distribution, if 50% or more customers has monetary value of 0 then the median will be 0. If we insisted on using mean the monetary value wil not be 0 but close to it, making it harder to read and interpret.

The following is the character for each of our segments.


```{python}

summary_1 = df_out.groupby('cluster').agg({'customer_id' : 'nunique',
                                           'length' : 'mean',
                                           'recency' : 'mean',
                                           'frequency' : 'mean',
                                           'monetary' : 'mean',
                                           'periodicity' : 'mean',
                                           'Mountain' : 'median',
                                           'Road' : 'median',
                                           'Standard' : 'median',
                                           'Touring' : 'median',
                                           'other' : 'median'
                                           })
                                           
summary_1['percent_member'] = summary_1['customer_id']/summary_1['customer_id'].sum()

summary_1.sort_values('customer_id', ascending = False)
```

Based on the product preference, we can identify several segments that buy certain product line. These 3 clusters has almost similar LRFMP metrics but differ in their product preference:

-   **Cluster 5: Buy Other Product**: This small segment is the only segment that has significant number of customers who purchased `Other` product line.
-   **Cluster 6 : Buy Mountain**: This segment of customer buy has significant number of customers who purchased `Mountain` product line.
-   **Cluster 7 : Buy Touring**: This segment of customer buy has significant number of customers who purchased `Touring` product line.

The above clusters has similar LRFMP model with the **Cluster 8: Most Loyal** with the only difference is that cluster 8 doesn't particulary has high number of customer who buy product line other than `Road` and `Standard`.

# Conclusion

We have learned how to identify retail customers using K-Means clustering algorithm using the LRFMP model and the customer's product purchases. Using the LRFMP model we can identify that we have a very loyal customers with high frequency of visit and also some customers who hasn't visit our store in a quite long time. By intregrating the LRFMP model and the customer's product purchases, we can identify customers in a more detailed segment, especially for their product preferences. This will be important for the marketing and the business as a whole since we can target specific customer based on their preference, thus giving them more personalized promotion and offers.
